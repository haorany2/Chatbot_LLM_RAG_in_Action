{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3a0339-467b-425b-a48a-d677ee61d0db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/custom_python/bin/tesseract'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd557b3-cc9b-4d3c-9333-d5ed0e353757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pdf2image\n",
    "# from pdf2image import convert_from_path\n",
    "# images = convert_from_path(\"data/tatr.pdf\", 500,\n",
    "#                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2b4872-4e73-45b5-8b32-af502c16b237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poppler_path=r'/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/custom_python/bin'\n",
    "TESSDATA_PREFIX = r'/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/custom_python/share/tessdata'\n",
    "env = os.environ.copy()\n",
    "\n",
    "if env.get(\"LD_LIBRARY_PATH\") is None:\n",
    "    os.environ[\"PATH\"] = poppler_path\n",
    "    sys.path.append(poppler_path)\n",
    "os.environ[\"TESSDATA_PREFIX\"] = TESSDATA_PREFIX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49505011-e4f9-4c32-92c9-228e90bec0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"env.txt\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_TEMPERATURE = float(os.getenv('OPENAI_TEMPERATURE'))\n",
    "HUGGING_FACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a543b5e-e8f8-4b91-b1d0-f92c12451806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-olIwOt3i6JDimGYiWr7uT3BlbkFJF8L8lCnH5wewnDko7ile'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4e469f-89ad-4c7d-95e1-c0af13f54490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os \n",
    "# for r,s,f in os.walk(\"/\"):\n",
    "#     for i in f:\n",
    "#         if \"tesseract\" in i:\n",
    "#             print(os.path.join(r,i))\n",
    "# !echo $TESSDATA_PREFIX\n",
    "# !export TESSDATA_PREFIX=/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/custom_python/share/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370112d6-5c97-4f5b-9ac7-301c92657012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    # return partition_pdf(\n",
    "    #     filename=path + fname,\n",
    "    #     extract_images_in_pdf=False,\n",
    "    #     infer_table_structure=True,\n",
    "    #     chunking_strategy=\"auto\",\n",
    "    #     max_characters=4000,\n",
    "    #     new_after_n_chars=3800,\n",
    "    #     combine_text_under_n_chars=2000,\n",
    "    #     image_output_dir_path=path,\n",
    "    # )\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,                  # mandatory\n",
    "        chunking_strategy=\"by_title\",\n",
    "        strategy=\"hi_res\",                                     # mandatory to use ``hi_res`` strategy\n",
    "        extract_images_in_pdf=True,                            # mandatory to set as ``True``\n",
    "        extract_image_block_types=[\"Image\", \"Table\"],          # optional\n",
    "        extract_image_block_to_payload=False,                  # optional\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        extract_image_block_output_dir=path + 'figures',  # optional - only works when ``extract_image_block_to_payload=False``\n",
    "       \n",
    "        )\n",
    "\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "# File path\n",
    "fpath = \"data/\"\n",
    "fname = \"tatr.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = extract_pdf_elements(fpath, fname)\n",
    "\n",
    "# Get text, tables\n",
    "texts, tables = categorize_elements(raw_pdf_elements)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c6074-2aed-4cbd-8f23-8fbaefe60394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f44cc7b-4a33-440c-a03f-1c2ec2c17425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\", openai_api_key=OPENAI_API_KEY)\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1a3be-104c-4c67-87f3-da41e5c8b05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(table_summaries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ffe6266-8567-462c-8a6b-9fa12913638d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Total Tables Investigated† Total Tables with a PRH∗ Total Tables with an oversegmented PRH % (of total with a PRH) % (of total investigated) SciTSR PubTabNet FinTabNet 10,431 422,491 70,028 342 100,159 25,637 54 58,747 25,348 15.79% 58.65% 98.87% 0.52% 13.90% 36.20% PubTables-1M (ours) 761,262 153,705 0 0% 0%\n"
     ]
    }
   ],
   "source": [
    "#print(tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac7804-20f5-4929-b5e5-851e13392efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(fpath + 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6c009aa-fac8-4c64-af10-4a2b34c41643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Email subscription form with a text field labeled \"Type your email...\" and a \"Subscribe\" button.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_summaries[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edaad6a-20a4-40f2-88b7-7cc82e87ec7a",
   "metadata": {},
   "source": [
    "### vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b740170-7ea4-4c2f-ab44-d24a62d0a4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag_cj_blog\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9961df5-e1cc-4e07-9d49-5ecfe7136a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVectorRetriever(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff7797e86a0>, docstore=<langchain_core.stores.InMemoryStore object at 0x7ff77a87af80>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_multi_vector_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac6a6b9-0c21-451e-b67d-ff6167a07f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a016f12b-2186-42d3-96dd-58c459279a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: MultiVectorRetriever(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff7797e86a0>, docstore=<langchain_core.stores.InMemoryStore object at 0x7ff77a87af80>)\n",
       "           | RunnableLambda(split_image_text_types),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| RunnableLambda(img_prompt_func)\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ff779a29f90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff791265180>, root_client=<openai.OpenAI object at 0x7ff77998d810>, root_async_client=<openai.AsyncOpenAI object at 0x7ff779a2a260>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_multimodal_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf758d12-bd38-4c61-918f-17d5c226db04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "query = \"Give me company names that are interesting investments based on EV / NTM and NTM rev growth. Consider EV / NTM multiples vs historical?\"\n",
    "docs = retriever_multi_vector_img.invoke(query, limit=6)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "283ac7d4-f143-45c9-9175-39c13aea9112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,source).\n",
       "\n",
       "This brings me back to the initial point - the two announcements from OpenAI I want to\n",
       "\n",
       "highlight here.\n",
       "\n",
       "1. Context length: Context window of GPT 4 Turbo went from 8k tokens to 128k tokens (think of this as ~300 pages of text worth of input). This means what you can put into a prompt just\n",
       "\n",
       "went up dramatically\n",
       "\n",
       "2. Costs decreasing: GPT 4 Turbo is 3x cheaper for input tokens (think of this as roughly the length of the prompt) and 2x cheaper for output tokens. This equates to $0.01 per 1k input tokens, and $0.03 per 1k output tokens. On a blended basis, GPT 4 Turbo is roughly 2.5-3x\n",
       "\n",
       "cheaper than GPT 4.\n",
       "\n",
       "The cost decrease is very meaningful - it’s lowers the barrier to experiment with AI, and also lowers the barrier for these AI functionalities to be pushed into production (because vendors don’t have to increase price nearly as much). Also - As Moin pointed out on Twitter / X, as context windows increase the need for task / domain-specic models (or ne-tuned models) decreases. The counter argument to this is will we be able to nd enough high quality long context training data. Either way - it’s clear these models are becoming cheaper and more eective, which is an exciting future for AI! I think we’re about to see an explosion of good\n",
       "\n",
       "business model AI applications in the near future. 2024 will be the year of AI applications!\n",
       "\n",
       "Datadog Gives Software the All Clear?\n",
       "\n",
       "Datadog Gives Soware the All Clear?\n",
       "\n",
       "This week soware stocks shot up on Tuesday, largely a result of Datadog’s quarterly earnings. Datadog in particular was up ~30%. So what happened? They made a number of comments about\n",
       "\n",
       "optimizations easing up, and the worst being behind us. Here are some quotes:\n",
       "\n",
       "“It looks like we've hit an inection point. It looks like there's a lot less overhang now in terms of what needs to be optimized or could be optimized by customers. It looks like\n",
       "\n",
       "also optimization is less intense and less widespread across the customer base.”\n",
       "\n",
       "“We had a very healthy start to Q4 in October...the trends we see in early Q4 are stronger\n",
       "\n",
       "than they've been for the past year.”\n",
       "\n",
       "“As we look at our overall customer activity, we continue to see customers optimizing but with less impact than we experienced in Q2, contributing to our usage growth with\n",
       "\n",
       "existing customers improving in Q3 relative to Q2.”\n",
       "\n",
       "“As a reminder, last quarter, we discussed a cohort of customers who began optimizing about a year ago and we said that they appear to stabilize their users growth at the end of Q2. That trend has held for the past several months with that cohorts usage remaining\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_img_base64(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffb6532d-31a8-4a0b-960c-237c975b3847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided charts and text, here are some investment insights and potential company names that might be interesting investments based on EV/NTM (Enterprise Value to Next Twelve Months) revenue multiples and NTM revenue growth:\\n\\n### Analysis of Charts:\\n1. **EV/NTM Revenue Multiples (First Chart)**:\\n   - **High Growth Median**: Currently at 11.8x, which is lower than the peak during the pandemic but still relatively high compared to historical averages.\\n   - **Mid Growth Median**: Currently at 7.4x, which is also lower than the peak but higher than pre-pandemic levels.\\n   - **Low Growth Median**: Currently at 3.9x, which is closer to historical averages.\\n\\n2. **Median EV/NTM Revenue/NTM Growth Multiples (Second Chart)**:\\n   - The median EV/NTM revenue to NTM growth multiple is currently at 0.38x, which is above the long-term average of 0.28x. This suggests that companies with higher growth rates are being valued more favorably compared to historical norms.\\n   - The 10-year Treasury yield is at 4.6%, which is relatively high and could impact valuations.\\n\\n3. **EV/NTM Revenue Multiples (Third Chart)**:\\n   - The median EV/NTM revenue multiple is at 7.8x, which is above the long-term pre-COVID average of 5.0x. This indicates that valuations are still elevated compared to historical norms.\\n\\n### Text Insights:\\n- **OpenAI**: The advancements in AI, particularly with GPT-4 Turbo, suggest that companies involved in AI and machine learning could see significant growth. The reduction in costs and increase in context length make AI applications more accessible and effective.\\n- **Datadog**: The positive earnings report and comments from Datadog indicate a potential recovery in the software sector. Datadog's stock surged by ~30%, suggesting strong investor confidence.\\n\\n### Potential Investment Candidates:\\n1. **Datadog (DDOG)**:\\n   - Given the positive earnings report and optimistic outlook, Datadog appears to be a strong candidate. The company is seeing less optimization pressure from customers and improved usage growth.\\n\\n2. **OpenAI-related Companies**:\\n   - Companies that are leveraging OpenAI's advancements or are heavily involved in AI and machine learning could be interesting investments. Examples include:\\n     - **Microsoft (MSFT)**: As a major investor in OpenAI and a leader in AI applications.\\n     - **NVIDIA (NVDA)**: A key player in providing the hardware (GPUs) necessary for AI computations.\\n\\n3. **High Growth Software Companies**:\\n   - Companies in the high growth median category with strong revenue growth and relatively high EV/NTM multiples could be attractive. Examples include:\\n     - **Snowflake (SNOW)**: Known for its high growth in the data warehousing space.\\n     - **CrowdStrike (CRWD)**: A leader in cybersecurity with strong growth prospects.\\n\\n### Conclusion:\\nBased on the EV/NTM revenue multiples and NTM revenue growth, companies like Datadog, Microsoft, NVIDIA, Snowflake, and CrowdStrike appear to be interesting investment opportunities. These companies are either benefiting from positive sector trends, like Datadog, or are positioned to capitalize on advancements in AI and high growth sectors. However, it's important to consider the elevated valuations compared to historical norms and the impact of higher interest rates on these valuations.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf0c14-f727-4cc2-869d-617570dbddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_python",
   "language": "python",
   "name": "custom_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
